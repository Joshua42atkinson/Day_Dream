# Gemma 3 Integration: The Hybrid Sovereign Model

## Overview

This document outlines the integration of **Google Gemma 3** into the "Ask Pete" platform, creating a "Hybrid Sovereign" architecture.

- **Local Sovereignty**: Gemma 3 (2B/270M) runs on the student's device (Edge AI).
- **Enterprise Scale**: **Antigravity** manages cloud synchronization and heavy reasoning via Gemini.

## Architecture

### Client-Side (The Edge)

- **Model**: Gemma 3 270M (WebGPU) or 2B (CPU/Metal).
- **Role**: Immediate feedback, "Coal" tracking, UI navigation, spell check.
- **Cost**: Zero marginal cost. Privacy preserved.

### Server-Side (The Bridge)

- **Model**: Gemma 3 27B (Optional Local Server) or Gemini Pro (Cloud).
- **Role**: Complex reasoning, Socratic dialogue generation, "Steam" verification.
- **Infrastructure**: Antigravity Bridge.

## The "Coal" Economy

- **Coal**: A metaphor for Compute Tokens.
  - Local Inference = Low Coal Cost.
  - Cloud Inference = High Coal Cost.
- **Steam**: A metaphor for Verified Mastery.
  - Generated by successfully applying knowledge.
  - Synced to Antigravity for institutional credit.

## Next Steps

1. **Download Models**:
   - `gemma-3-2b-it-quantized.gguf`
   - `gemma-3-270m-it-webgpu`
2. **Configure Backend**:
   - Update `backend/src/ai/llm/gemma_engine.rs`.
3. **Configure Frontend**:
   - Update `frontend/src/ai/gemma_agent.rs`.
